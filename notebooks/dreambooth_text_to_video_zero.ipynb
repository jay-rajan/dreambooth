{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrajan/micromamba/envs/diffusers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.93it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoZeroPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute '__module__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jrajan/dreambooth/models/pointling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mTextToVideoZeroPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# prompt = \"A photo of a pntlng animation playing guitar on times square\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# result = pipe(prompt=prompt).images\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# result = [(r * 255).astype(\"uint8\") for r in result]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# imageio.mimsave(\"video.mp4\", result, fps=4)\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1357\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1354\u001b[0m     )\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;66;03m# 8. Instantiate the pipeline\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# 9. Save where the model was instantiated from\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m model\u001b[38;5;241m.\u001b[39mregister_to_config(_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path)\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py:314\u001b[0m, in \u001b[0;36mTextToVideoZeroPipeline.__init__\u001b[0;34m(self, vae, text_encoder, tokenizer, unet, scheduler, safety_checker, feature_extractor, requires_safety_checker)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    305\u001b[0m     vae: AutoencoderKL,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     requires_safety_checker: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m ):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_checker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_safety_checker\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     processor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    318\u001b[0m         CrossFrameAttnProcessor2_0(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(F, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_dot_product_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m CrossFrameAttnProcessor(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet\u001b[38;5;241m.\u001b[39mset_attn_processor(processor)\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:236\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__init__\u001b[0;34m(self, vae, text_encoder, tokenizer, unet, scheduler, safety_checker, feature_extractor, image_encoder, requires_safety_checker)\u001b[0m\n\u001b[1;32m    233\u001b[0m     new_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m    234\u001b[0m     unet\u001b[38;5;241m.\u001b[39m_internal_dict \u001b[38;5;241m=\u001b[39m FrozenDict(new_config)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_checker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_checker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_scale_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_out_channels) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor \u001b[38;5;241m=\u001b[39m VaeImageProcessor(vae_scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_scale_factor)\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:566\u001b[0m, in \u001b[0;36mDiffusionPipeline.register_modules\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;66;03m# register the config from the original module, not the dynamo compiled one\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     not_compiled_module \u001b[38;5;241m=\u001b[39m _unwrap_model(module)\n\u001b[0;32m--> 566\u001b[0m     library \u001b[38;5;241m=\u001b[39m \u001b[43mnot_compiled_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;66;03m# check if the module is a pipeline module\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     module_path_items \u001b[38;5;241m=\u001b[39m not_compiled_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute '__module__'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import TextToVideoZeroPipeline\n",
    "import imageio\n",
    "\n",
    "model_id = \"/home/jrajan/dreambooth/models/pointling\"\n",
    "pipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# prompt = \"A photo of a pntlng animation playing guitar on times square\"\n",
    "# result = pipe(prompt=prompt).images\n",
    "# result = [(r * 255).astype(\"uint8\") for r in result]\n",
    "# imageio.mimsave(\"video.mp4\", result, fps=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  8.39it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  8.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:39<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"An image of a pntlng animation swimming in the beach\"\n",
    "result = pipe(prompt=prompt).images\n",
    "result = [(r * 255).astype(\"uint8\") for r in result]\n",
    "imageio.mimsave(\"video.mp4\", result, fps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  5.89it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.TextToVideoZeroPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute '__module__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jrajan/dreambooth/models/pointling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mTextToVideoZeroPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn image of a pntlng animation character walking a dog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1357\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1353\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1354\u001b[0m     )\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;66;03m# 8. Instantiate the pipeline\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# 9. Save where the model was instantiated from\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m model\u001b[38;5;241m.\u001b[39mregister_to_config(_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path)\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py:314\u001b[0m, in \u001b[0;36mTextToVideoZeroPipeline.__init__\u001b[0;34m(self, vae, text_encoder, tokenizer, unet, scheduler, safety_checker, feature_extractor, requires_safety_checker)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    305\u001b[0m     vae: AutoencoderKL,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     requires_safety_checker: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m ):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_checker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_safety_checker\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     processor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    318\u001b[0m         CrossFrameAttnProcessor2_0(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(F, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_dot_product_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m CrossFrameAttnProcessor(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet\u001b[38;5;241m.\u001b[39mset_attn_processor(processor)\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:236\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__init__\u001b[0;34m(self, vae, text_encoder, tokenizer, unet, scheduler, safety_checker, feature_extractor, image_encoder, requires_safety_checker)\u001b[0m\n\u001b[1;32m    233\u001b[0m     new_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m    234\u001b[0m     unet\u001b[38;5;241m.\u001b[39m_internal_dict \u001b[38;5;241m=\u001b[39m FrozenDict(new_config)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_checker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_checker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_scale_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_out_channels) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor \u001b[38;5;241m=\u001b[39m VaeImageProcessor(vae_scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae_scale_factor)\n",
      "File \u001b[0;32m~/micromamba/envs/diffusers/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:566\u001b[0m, in \u001b[0;36mDiffusionPipeline.register_modules\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;66;03m# register the config from the original module, not the dynamo compiled one\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     not_compiled_module \u001b[38;5;241m=\u001b[39m _unwrap_model(module)\n\u001b[0;32m--> 566\u001b[0m     library \u001b[38;5;241m=\u001b[39m \u001b[43mnot_compiled_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;66;03m# check if the module is a pipeline module\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     module_path_items \u001b[38;5;241m=\u001b[39m not_compiled_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute '__module__'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import TextToVideoZeroPipeline\n",
    "import numpy as np\n",
    "\n",
    "model_id = \"/home/jrajan/dreambooth/models/pointling\"\n",
    "pipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "prompt = \"An image of a pntlng animation character walking a dog\"\n",
    "\n",
    "seed = 0\n",
    "video_length = 24  #24 ÷ 4fps = 6 seconds\n",
    "chunk_size = 8\n",
    "\n",
    "# Generate the video chunk-by-chunk\n",
    "result = []\n",
    "chunk_ids = np.arange(0, video_length, chunk_size - 1)\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "for i in range(len(chunk_ids)):\n",
    "    print(f\"Processing chunk {i + 1} / {len(chunk_ids)}\")\n",
    "    ch_start = chunk_ids[i]\n",
    "    ch_end = video_length if i == len(chunk_ids) - 1 else chunk_ids[i + 1]\n",
    "    # Attach the first frame for Cross Frame Attention\n",
    "    frame_ids = [0] + list(range(ch_start, ch_end))\n",
    "    # Fix the seed for the temporal consistency\n",
    "    generator.manual_seed(seed)\n",
    "    output = pipe(prompt=prompt, video_length=len(frame_ids), generator=generator, frame_ids=frame_ids)\n",
    "    result.append(output.images[1:])\n",
    "\n",
    "# Concatenate chunks and save\n",
    "result = np.concatenate(result)\n",
    "result = [(r * 255).astype(\"uint8\") for r in result]\n",
    "imageio.mimsave(\"video.mp4\", result, fps=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrajan/micromamba/envs/diffusers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.24.0.dev0\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-2-1\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"requires_safety_checker\": false,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DPMSolverMultistepScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import torch\n",
    "\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrajan/micromamba/envs/diffusers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jrajan/dreambooth/models/pointling_lora\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m TextToVideoZeroPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-2-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m pipe\u001b[38;5;241m.\u001b[39munet\u001b[38;5;241m.\u001b[39mload_attn_procs(model_path)\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA photo of a pntlng animation waving at eiffel tower\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from diffusers import TextToVideoZeroPipeline\n",
    "import imageio\n",
    "model_path = \"/home/jrajan/dreambooth/models/pointling_lora\"\n",
    "pipe = TextToVideoZeroPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.unet.load_attn_procs(model_path)\n",
    "\n",
    "prompt = \"A photo of a pntlng animation waving at eiffel tower\"\n",
    "result = pipe(prompt=prompt).images\n",
    "result = [(r * 255).astype(\"uint8\") for r in result]\n",
    "imageio.mimsave(\"video.mp4\", result, fps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:42<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:42<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:42<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 4 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:21<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import TextToVideoZeroPipeline\n",
    "import numpy as np\n",
    "\n",
    "prompt = \"A photo of a pntlng animation waving at eiffel tower\"\n",
    "seed = 0\n",
    "video_length = 24  #24 ÷ 4fps = 6 seconds\n",
    "chunk_size = 8\n",
    "\n",
    "# Generate the video chunk-by-chunk\n",
    "result = []\n",
    "chunk_ids = np.arange(0, video_length, chunk_size - 1)\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "for i in range(len(chunk_ids)):\n",
    "    print(f\"Processing chunk {i + 1} / {len(chunk_ids)}\")\n",
    "    ch_start = chunk_ids[i]\n",
    "    ch_end = video_length if i == len(chunk_ids) - 1 else chunk_ids[i + 1]\n",
    "    # Attach the first frame for Cross Frame Attention\n",
    "    frame_ids = [0] + list(range(ch_start, ch_end))\n",
    "    # Fix the seed for the temporal consistency\n",
    "    generator.manual_seed(seed)\n",
    "    output = pipe(prompt=prompt, video_length=len(frame_ids), generator=generator, frame_ids=frame_ids)\n",
    "    result.append(output.images[1:])\n",
    "\n",
    "# Concatenate chunks and save\n",
    "result = np.concatenate(result)\n",
    "result = [(r * 255).astype(\"uint8\") for r in result]\n",
    "imageio.mimsave(\"video.mp4\", result, fps=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
